import 'dotenv/config';

import bodyParser from 'body-parser';
import express, { type Request, type Response } from 'express';
import fs from 'node:fs';
import path from 'node:path';
import crypto from 'node:crypto';
import OpenAI from 'openai';
import WebSocket from 'ws';
import twilio from 'twilio';

// ─── Security Helpers ───

/**
 * Verify OpenAI webhook signature using HMAC-SHA256.
 * OpenAI sends header: openai-signature containing v1=<hex-encoded HMAC-SHA256>
 */
function verifyWebhookSignature(rawBody: Buffer, signatureHeader: string, secret: string): boolean {
  try {
    if (!signatureHeader || !secret) return false;

    // Extract v1=... value from header
    const match = signatureHeader.match(/v1=([a-f0-9]+)/);
    if (!match) return false;
    const providedSignature = match[1];

    // Compute HMAC-SHA256 of raw body
    const hmac = crypto.createHmac('sha256', secret);
    hmac.update(rawBody);
    const computedSignature = hmac.digest('hex');

    // Use timing-safe comparison
    if (providedSignature.length !== computedSignature.length) return false;
    return crypto.timingSafeEqual(
      Buffer.from(providedSignature, 'hex'),
      Buffer.from(computedSignature, 'hex')
    );
  } catch {
    return false;
  }
}

/**
 * Validate that a path stays within the expected base directory.
 * Throws if path escapes the base.
 */
function safePath(basePath: string, userPath: string): string {
  const base = path.resolve(basePath);
  const resolved = path.resolve(base, userPath);

  if (!resolved.startsWith(base + path.sep) && resolved !== base) {
    throw new Error(`Path traversal attempt blocked: ${userPath}`);
  }

  return resolved;
}

/**
 * Sanitize environment variable values used in LLM prompts.
 * Strips dangerous characters and truncates to prevent prompt injection.
 */
function sanitizeEnvName(value: string, maxLen = 50): string {
  if (!value) return '';
  // Allow alphanumeric, spaces, hyphens, apostrophes, periods
  const cleaned = value.replace(/[^a-zA-Z0-9\s'\-\.]/g, '');
  return cleaned.slice(0, maxLen);
}

const PORT = Number(process.env.PORT ?? 8000);
const PUBLIC_BASE_URL = mustGetEnv('PUBLIC_BASE_URL');

const TWILIO_ACCOUNT_SID = mustGetEnv('TWILIO_ACCOUNT_SID');
const TWILIO_AUTH_TOKEN = mustGetEnv('TWILIO_AUTH_TOKEN');
const TWILIO_CALLER_ID = mustGetEnv('TWILIO_CALLER_ID');

const OPENAI_API_KEY = mustGetEnv('OPENAI_API_KEY');
const OPENAI_PROJECT_ID = mustGetEnv('OPENAI_PROJECT_ID');
const OPENAI_WEBHOOK_SECRET = mustGetEnv('OPENAI_WEBHOOK_SECRET');
const OPENAI_VOICE = process.env.OPENAI_VOICE ?? 'alloy';

// OpenClaw gateway for assistant brain-in-loop (Phase C2)
const OPENCLAW_GATEWAY_URL = process.env.OPENCLAW_GATEWAY_URL ?? 'http://127.0.0.1:18789';
const OPENCLAW_GATEWAY_TOKEN = process.env.OPENCLAW_GATEWAY_TOKEN ?? '';

// Configurable operator/assistant info (sanitized to prevent prompt injection)
const ASSISTANT_NAME = sanitizeEnvName(process.env.ASSISTANT_NAME ?? 'Amber');
const OPERATOR_NAME = sanitizeEnvName(process.env.OPERATOR_NAME ?? 'your operator');
const OPERATOR_PHONE = process.env.OPERATOR_PHONE ?? '';
const OPERATOR_EMAIL = process.env.OPERATOR_EMAIL ?? '';
const ORG_NAME = process.env.ORG_NAME ?? '';
const DEFAULT_CALENDAR = process.env.DEFAULT_CALENDAR ?? '';

// Configurable GenZ caller numbers (comma-separated E.164 numbers)
const GENZ_NUMBERS = (process.env.GENZ_CALLER_NUMBERS ?? '').split(',').map(s => s.trim()).filter(Boolean);

// Configurable outbound map path (validated to prevent path traversal)
const OUTBOUND_MAP_PATH = (() => {
  const userPath = process.env.OUTBOUND_MAP_PATH;
  const defaultPath = path.join(process.cwd(), 'data', 'bridge-outbound-map.json');
  
  if (!userPath) return defaultPath;
  
  try {
    return safePath(process.cwd(), userPath);
  } catch (e) {
    console.warn(`OUTBOUND_MAP_PATH validation failed (${userPath}), using default:`, e instanceof Error ? e.message : String(e));
    return defaultPath;
  }
})();

// ─── Phase C2: OpenClaw brain-in-loop tool definitions ───
const OPENCLAW_TOOLS = [
  {
    type: 'function' as const,
    name: 'ask_openclaw',
    description: [
      "Ask the OpenClaw assistant for information you don't have.",
      'Use this when the person on the call asks something you cannot answer from your instructions alone.',
      'Examples: checking calendar availability, looking up contact info, getting preferences,',
      'or any question that requires context beyond this call.',
      'Keep your question concise and specific.',
    ].join(' '),
    parameters: {
      type: 'object',
      properties: {
        question: {
          type: 'string',
          description: 'The specific question to ask the assistant. Be concise.',
        },
        context: {
          type: 'string',
          description: 'Brief context about why you need this info (what the caller asked).',
        },
      },
      required: ['question'],
    },
  },
];

// Active call websockets keyed by callId, so we can interact with them
const activeCallSockets = new Map<string, WebSocket>();

const app = express();

type InboundCallInfo = {
  callSid: string;
  from: string;
  receivedAtMs: number;
};

type InboundCallScreeningStyle = 'friendly' | 'genz';

const INBOUND_CALL_LOOKBACK_MS = 2 * 60 * 1000;
const INBOUND_CALL_RETENTION_MS = 10 * 60 * 1000;

const inboundCallBySid = new Map<string, InboundCallInfo>();

// JSON endpoints
app.use('/call', express.json());
app.use('/openclaw', express.json());

// Stores the desired objective/intent for the next outbound call(s), so the OpenAI
// webhook can pick the right instructions when the SIP leg arrives.
// Note: this is in-memory only (resets on restart). Good enough for quick tests.
type CallPlan = {
  purpose?: string;           // e.g. "restaurant_reservation", "inquiry", "appointment"
  restaurantName?: string;
  date?: string;              // YYYY-MM-DD
  time?: string;              // e.g. "7:00 PM"
  partySize?: number;
  notes?: string;             // e.g. "patio if possible", "birthday dinner"
  customer?: {
    name?: string;
    phone?: string;
    email?: string;
  };
};

type OutboundIntent = {
  key: string; // can be twilio CallSid or our bridgeId
  objective: string;
  callPlan?: CallPlan;
  createdAtMs: number;
};
const OUTBOUND_INTENT_RETENTION_MS = 10 * 60 * 1000;
const outboundIntentByKey = new Map<string, OutboundIntent>();

// Twilio callbacks are typically form-encoded
app.use('/twilio', express.urlencoded({ extended: false }));
app.use('/twiml', express.urlencoded({ extended: false }));

// Raw body for webhook verification (OpenAI expects raw bytes)
app.use('/openai/webhook', bodyParser.raw({ type: '*/*' }));

const twilioClient = twilio(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN);
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

app.get('/healthz', (_req, res) => res.status(200).json({ ok: true }));

/**
 * POST /openclaw/ask
 * Test endpoint — manually ask the assistant a question (useful for debugging C2).
 * Body: { question: "...", context?: "...", objective?: "...", callPlan?: {...} }
 */
app.post('/openclaw/ask', async (req: Request, res: Response) => {
  try {
    const question = String(req.body?.question ?? '').trim();
    if (!question) return res.status(400).json({ error: 'Missing question' });

    const answer = await askOpenClaw(question, {
      callPlan: req.body?.callPlan,
      objective: req.body?.objective,
      transcript: req.body?.transcript,
    });

    return res.status(200).json({ answer });
  } catch (e: any) {
    return res.status(500).json({ error: e?.message ?? String(e) });
  }
});

/**
 * POST /twilio/status
 * Receives Twilio call status callbacks to help debug early hangups / SIP failures.
 */
app.post('/twilio/status', (req: Request, res: Response) => {
  try {
    const b = req.body as any;
    rememberInboundCallFromTwilioBody(b);
    console.log('TWILIO_STATUS', {
      CallSid: b?.CallSid,
      CallStatus: b?.CallStatus,
      Timestamp: b?.Timestamp,
      From: b?.From,
      To: b?.To,
      Direction: b?.Direction,
      ApiVersion: b?.ApiVersion,
      AccountSid: b?.AccountSid,
      // Common debug fields
      Caller: b?.Caller,
      Called: b?.Called,
      CallDuration: b?.CallDuration,
      // These are sometimes included for <Dial><Sip> failures
      SipResponseCode: b?.SipResponseCode,
      SipCallId: b?.SipCallId,
      DialCallStatus: b?.DialCallStatus,
      DialCallSid: b?.DialCallSid,
    });
  } catch (e) {
    console.error('TWILIO_STATUS parse error', e);
  }
  return res.sendStatus(204);
});

/**
 * POST /twilio/inbound
 * Returns TwiML that bridges inbound PSTN calls (to your Twilio number) to OpenAI Realtime SIP.
 */
app.post('/twilio/inbound', async (req: Request, res: Response) => {
  try {
    rememberInboundCallFromTwilioBody(req.body as any);
  } catch (e) {
    console.error('TWILIO_INBOUND remember error', e);
  }
  res.type('text/xml').status(200).send(buildOpenAiSipBridgeTwiML());
});

/**
 * POST /call/outbound
 * Body: { to: "+1..." } (E.164)
 * Creates a Twilio outbound PSTN call. When the callee answers, Twilio will request TwiML from /twiml/bridge.
 */
app.post('/call/outbound', async (req: Request, res: Response) => {
  try {
    const to = String(req.body?.to ?? '').trim();
    if (!isE164(to)) {
      return res.status(400).json({ error: 'Invalid `to`. Expected E.164 string like +14165551234' });
    }

    const statusCallback = new URL('/twilio/status', PUBLIC_BASE_URL).toString();

    const objective = String(req.body?.objective ?? req.body?.intent ?? '').trim();
    const callPlan: CallPlan | undefined = req.body?.callPlan && typeof req.body.callPlan === 'object'
      ? req.body.callPlan as CallPlan
      : undefined;

    // Create a stable bridge id so we can correlate the OpenAI SIP leg back to this outbound request.
    const bridgeId = `b_${Date.now()}_${Math.random().toString(16).slice(2)}`;

    const urlObj = new URL('/twiml/bridge', PUBLIC_BASE_URL);
    urlObj.searchParams.set('bridge_id', bridgeId);
    const url = urlObj.toString();

    const call = await twilioClient.calls.create({
      to,
      from: TWILIO_CALLER_ID,
      url,
      method: 'POST',
      statusCallback,
      statusCallbackMethod: 'POST',
      statusCallbackEvent: ['initiated', 'ringing', 'answered', 'completed']
    });

    // Persist outbound map so downstream log sync can show the dialed PSTN number.
    // Schema: { schema:1, updatedAt, calls:{ [twilioCallSid]: { to, bridgeId, objective, createdAtMs } } }
    try {
      const mapPath = OUTBOUND_MAP_PATH;
      ensureDirSync(path.dirname(mapPath));
      let existing: any = { schema: 1, updatedAt: new Date().toISOString(), calls: {} };
      if (fs.existsSync(mapPath)) {
        try { existing = JSON.parse(fs.readFileSync(mapPath, 'utf8')); } catch {}
        if (!existing || typeof existing !== 'object') existing = { schema: 1, calls: {} };
        if (!existing.calls || typeof existing.calls !== 'object') existing.calls = {};
      }
      existing.calls[call.sid] = {
        to,
        bridgeId,
        objective: objective || null,
        callPlan: callPlan || null,
        createdAtMs: Date.now()
      };
      existing.updatedAt = new Date().toISOString();
      fs.writeFileSync(mapPath, JSON.stringify(existing, null, 2) + '\n', 'utf8');
    } catch (e) {
      console.error('Failed to persist bridge-outbound-map.json', e);
    }

    if (objective || callPlan) {
      // Store under bridgeId so we can correlate even if Twilio emits a different CallSid on the SIP leg.
      outboundIntentByKey.set(bridgeId, {
        key: bridgeId,
        objective: objective || callPlanToObjective(callPlan),
        callPlan,
        createdAtMs: Date.now()
      });
    }

    return res.status(200).json({ sid: call.sid, status: call.status });
  } catch (err: any) {
    return res.status(500).json({ error: err?.message ?? String(err) });
  }
});

/**
 * POST /twiml/bridge
 * Returns TwiML that bridges the live call to OpenAI Realtime SIP connector.
 */
app.post('/twiml/bridge', async (req: Request, res: Response) => {
  const callSid = normalizePhoneLike((req.body as any)?.CallSid);
  const bridgeId = normalizePhoneLike((req.query as any)?.bridge_id);

  const objective = bridgeId ? (outboundIntentByKey.get(bridgeId)?.objective ?? '') : '';

  res.type('text/xml')
    .status(200)
    .send(buildOpenAiSipBridgeTwiML({ callSid, bridgeId, objective }));
});

/**
 * POST /openai/webhook
 * OpenAI sends realtime.call.incoming here when a SIP call arrives at your OpenAI project.
 *
 * We verify the signature with OPENAI_WEBHOOK_SECRET using the OpenAI SDK.
 * Then we:
 * 1) Accept the call via REST: POST /v1/realtime/calls/{call_id}/accept
 * 2) Connect a websocket with ?call_id=... and send a response.create to speak a greeting.
 */
app.post('/openai/webhook', async (req: Request, res: Response) => {
  try {
    // Verify webhook signature (CRITICAL security requirement)
    const signatureHeader = req.headers['openai-signature'] as string | undefined;
    const rawBody = req.body as Buffer;

    if (!verifyWebhookSignature(rawBody, signatureHeader || '', OPENAI_WEBHOOK_SECRET)) {
      console.error('WEBHOOK_SIGNATURE_VERIFICATION_FAILED', {
        hasSignature: !!signatureHeader,
        hasSecret: !!OPENAI_WEBHOOK_SECRET,
        bodyLength: rawBody.length
      });
      return res.status(401).send('Unauthorized: Invalid signature');
    }

    const event = JSON.parse(rawBody.toString('utf8'));

    if (event?.type !== 'realtime.call.incoming') {
      return res.sendStatus(200);
    }

    const callId: string | undefined = event?.data?.call_id;

    // Debug: persist raw incoming event so we can see what OpenAI provides (SIP headers/metadata)
    try {
      ensureDirSync(logsDir());
      const incomingEventPath = path.join(logsDir(), 'incoming_' + Date.now() + '.realtime.call.incoming.json');
      fs.writeFileSync(incomingEventPath, JSON.stringify(event, null, 2));
      console.log('WROTE_INCOMING_EVENT', incomingEventPath);
    } catch (e) {
      console.error('Failed to write incoming event', e);
    }

    if (!callId) {
      return res.status(400).send('Missing call_id');
    }

    const nowMs = Date.now();

    const inbound = findRecentInboundCallForWebhook(event, nowMs);
    const style = selectInboundCallScreeningStyle(inbound?.from);

    const twilioSid = extractTwilioCallSidFromWebhook(event);
    const bridgeId = extractBridgeIdFromWebhook(event);

    // Prefer objective passed via SIP header (x_objective) since that will be present on the SIP INVITE.
    // Fall back to our in-memory map keyed by bridge id.
    const objectiveFromHeader = extractObjectiveFromWebhook(event);
    const outboundObjective = objectiveFromHeader?.trim()
      ? objectiveFromHeader.trim()
      : (bridgeId ? getOutboundObjectiveForKey(bridgeId, nowMs) : undefined);

    const outboundCallPlan = bridgeId ? getOutboundCallPlanForKey(bridgeId, nowMs) : undefined;

    const instructions = outboundObjective
      ? buildOutboundCallInstructions({ objective: outboundObjective, callPlan: outboundCallPlan })
      : buildInboundCallScreeningInstructions({ style });

    

const callAccept = {
      instructions,
      type: 'realtime',
      model: 'gpt-realtime',
      tools: OPENCLAW_TOOLS,
      tool_choice: 'auto',
      audio: {
        output: { voice: OPENAI_VOICE },
        // Enable caller-side transcription (no `enabled` flag; schema expects a model)
        input: { transcription: { model: 'gpt-4o-mini-transcribe' } }
      }
    } as const;

    const acceptResp = await fetch(
      `https://api.openai.com/v1/realtime/calls/${encodeURIComponent(callId)}/accept`,
      {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(callAccept)
      }
    );

    if (!acceptResp.ok) {
      const text = await acceptResp.text().catch(() => '');
      console.error('ACCEPT failed:', acceptResp.status, acceptResp.statusText, text);
      return res.status(500).send('Accept failed');
    }

    ensureDirSync(logsDir());
    const jsonlStream = fs.createWriteStream(logsJsonlPath(callId), { flags: 'a' });
    const transcriptStream = fs.createWriteStream(logsTranscriptPath(callId), { flags: 'a' });

    const writeJsonl = (obj: unknown) => {
      jsonlStream.write(`${JSON.stringify(obj)}\n`);
    };

    // Debug: log what we think this call is (outbound objective vs inbound screening)
    const twilioSidForDebug = extractTwilioCallSidFromWebhook(event);
    writeJsonl({
      type: 'call.intent',
      received_at: new Date().toISOString(),
      twilio_call_sid: twilioSidForDebug ?? null,
      outbound_objective: outboundObjective ?? null,
      selected_mode: outboundObjective ? 'outbound' : 'inbound',
      inbound_from: inbound?.from ?? null,
      inbound_style: style
    });


    // Send a greeting over the call websocket.
    const greeting = outboundObjective
      ? buildOutboundGreeting({ objective: outboundObjective })
      : buildInboundGreeting({ style });
    const wssUrl = `wss://api.openai.com/v1/realtime?call_id=${encodeURIComponent(callId)}`;

    const ws = new WebSocket(wssUrl, {
      headers: {
        Authorization: `Bearer ${OPENAI_API_KEY}`,
        origin: 'https://api.openai.com'
      }
    });

    // Track this socket so we can reference it later
    activeCallSockets.set(callId, ws);

    ws.on('open', () => {
      writeJsonl({ type: 'ws.open', call_id: callId, received_at: new Date().toISOString() });

      // Phase C2: Re-register tools + VAD tuning via session.update
      {
        const sessionUpdate = {
          type: 'session.update',
          session: {
            tools: OPENCLAW_TOOLS,
            tool_choice: 'auto',
            turn_detection: {
              type: 'server_vad',
              threshold: 0.7,
              prefix_padding_ms: 500,
              silence_duration_ms: 700,
            },
          },
        };
        ws.send(JSON.stringify(sessionUpdate));
        writeJsonl({ type: 'c2.tools_registered', call_id: callId, received_at: new Date().toISOString(), toolCount: OPENCLAW_TOOLS.length });
      }

      const responseCreate = {
        type: 'response.create',
        response: { instructions: `Say to the user: ${greeting}` }
      } as const;
      ws.send(JSON.stringify(responseCreate));

      // If the caller is silent, send a single gentle follow-up after ~3 seconds.
      // Cancel the follow-up once we see any user transcript/text.
      let followupDone = false;
      const followupText = buildSilenceFollowup({ mode: outboundObjective ? 'outbound' : 'inbound' });

      const followupTimer = setTimeout(() => {
        if (followupDone) return;
        followupDone = true;

        const followupCreate = {
          type: 'response.create',
          response: { instructions: `Say to the user: ${followupText}` }
        } as const;

        try {
          ws.send(JSON.stringify(followupCreate));
          writeJsonl({
            type: 'silence.followup.sent',
            at: new Date().toISOString(),
            mode: outboundObjective ? 'outbound' : 'inbound'
          });
        } catch {
          writeJsonl({ type: 'silence.followup.error', at: new Date().toISOString() });
        }
      }, 3000);

      const cancelFollowup = () => {
        if (followupDone) return;
        followupDone = true;
        clearTimeout(followupTimer);
        writeJsonl({ type: 'silence.followup.cancelled', at: new Date().toISOString() });
      };

      // Cancel on any inbound websocket event that includes transcript/text.
      ws.on('message', (data) => {
        if (followupDone) return;
        try {
          const raw =
            typeof data === 'string'
              ? data
              : Buffer.isBuffer(data)
                ? data.toString('utf8')
                : Buffer.from(data as any).toString('utf8');
          const parsed = JSON.parse(raw);
          if (extractTranscriptStrings(parsed).length) cancelFollowup();
        } catch {
          // ignore
        }
      });
    });

    // Accumulate function call arguments (they may come in deltas)
    const pendingFunctionCalls = new Map<string, { name: string; args: string }>();

    ws.on('message', (data, isBinary) => {
      const receivedAt = new Date().toISOString();
      const rawText =
        typeof data === 'string'
          ? data
          : Buffer.isBuffer(data)
            ? data.toString('utf8')
            : Buffer.from(data as any).toString('utf8');
      let parsed: any = undefined;
      try {
        parsed = JSON.parse(rawText);
      } catch {
        parsed = { type: 'unparsed', raw: rawText };
      }

      writeJsonl({ received_at: receivedAt, event: parsed });

      for (const t of extractTranscriptStrings(parsed)) {
        transcriptStream.write(`${t}\n`);
      }

      // Phase C2: Handle function call events
      const eventType = String(parsed?.type ?? '');

      // Track function call argument deltas
      if (eventType === 'response.function_call_arguments.delta') {
        const itemId = parsed?.item_id ?? parsed?.call_id ?? 'unknown';
        const existing = pendingFunctionCalls.get(itemId);
        if (existing) {
          existing.args += parsed?.delta ?? '';
        } else {
          pendingFunctionCalls.set(itemId, {
            name: parsed?.name ?? '',
            args: parsed?.delta ?? '',
          });
        }
      }

      // Function call is complete — execute it
      if (eventType === 'response.function_call_arguments.done') {
        const itemId = parsed?.item_id ?? 'unknown';
        const fnCallId = parsed?.call_id ?? itemId;
        const fnName = parsed?.name ?? pendingFunctionCalls.get(itemId)?.name ?? '';
        const fnArgs = parsed?.arguments ?? pendingFunctionCalls.get(itemId)?.args ?? '{}';
        pendingFunctionCalls.delete(itemId);

        writeJsonl({
          type: 'c2.function_call_detected',
          call_id: callId,
          received_at: new Date().toISOString(),
          fn_name: fnName,
          fn_args: fnArgs,
          item_id: itemId,
        });

        if (fnName === 'ask_openclaw') {
          // Inject verbal filler BEFORE processing so the caller isn't waiting in silence.
          // We send a response.create with a brief "checking" message. The Realtime API
          // will generate this speech while we async-process the function call.
          const fillerMsg = {
            type: 'response.create',
            response: {
              instructions: 'Say briefly and naturally: "One moment, let me check on that for you."',
            },
          };
          ws.send(JSON.stringify(fillerMsg));
          writeJsonl({ type: 'c2.filler_sent', call_id: callId, received_at: new Date().toISOString() });

          handleAskOpenClaw(ws, callId, itemId, fnCallId, fnArgs, outboundObjective, outboundCallPlan, transcriptStream, writeJsonl);
        } else {
          // Unknown function — return a generic error
          sendFunctionCallOutput(ws, fnCallId, JSON.stringify({ error: `Unknown function: ${fnName}` }));
        }
      }
    });

    ws.on('error', (e: unknown) => {
      console.error('WebSocket error:', e);
      writeJsonl({
        type: 'ws.error',
        call_id: callId,
        received_at: new Date().toISOString(),
        error: e instanceof Error ? { name: e.name, message: e.message, stack: e.stack } : e
      });
    });

    ws.on('close', async (code, reason) => {
      activeCallSockets.delete(callId);
      writeJsonl({
        type: 'ws.close',
        call_id: callId,
        received_at: new Date().toISOString(),
        code,
        reason: reason.toString()
      });
      await Promise.all([endStream(jsonlStream), endStream(transcriptStream)]);
      await finalizeSummaryFromTranscript(callId);
    });

    // Always ack the webhook quickly.
    return res.sendStatus(200);
  } catch (e: any) {
    console.error('Webhook error:', e);
    return res.status(500).send('Server error');
  }
});

app.listen(PORT, () => {
  console.log(`twilio-openai-sip-bridge listening on http://127.0.0.1:${PORT}`);
});

function mustGetEnv(name: string): string {
  const v = process.env[name];
  if (!v) throw new Error(`Missing required env var: ${name}`);
  return v;
}

function isE164(s: string): boolean {
  return /^\+[1-9]\d{1,14}$/.test(s);
}

function buildOpenAiSipBridgeTwiML(args?: { callSid?: string; bridgeId?: string; objective?: string }): string {
  const twiml = new twilio.twiml.VoiceResponse();
  const dial = twiml.dial({ answerOnBridge: true });

  // IMPORTANT: Put correlation fields in the SIP URI *parameters* (after the userpart/host),
  // not in the query string. Query params are not reliably surfaced as SIP headers.
  // NOTE: Do NOT put the full objective here — long SIP URIs break the INVITE.
  // The objective is resolved from the in-memory map via bridgeId in the webhook handler.
  const uriParams: string[] = ['transport=tls'];
  if (args?.callSid) uriParams.push(`x_twilio_callsid=${encodeURIComponent(args.callSid)}`);
  if (args?.bridgeId) uriParams.push(`x_bridge_id=${encodeURIComponent(args.bridgeId)}`);

  const uri = `sip:${OPENAI_PROJECT_ID}@sip.api.openai.com;${uriParams.join(';')}`;

  dial.sip(uri);
  return twiml.toString();
}

function buildInboundCallScreeningInstructions(args: { style: InboundCallScreeningStyle }): string {
  const styleRules =
    args.style === 'genz'
      ? [
          "Style: Gen Z-ish, playful, warm.",
          "Keep it natural (not cringey), still respectful and clear.",
          "Use light slang sparingly (e.g., 'hey', 'gotcha', 'all good')."
        ]
      : [
          'Style: friendly, casual, professional.',
          'Sound warm and personable, but keep it efficient.',
          "Avoid slang that's too heavy or jokey."
        ];

  const assistantIntro = OPERATOR_NAME
    ? `You are ${OPERATOR_NAME}'s assistant answering an inbound phone call on ${OPERATOR_NAME}'s behalf.`
    : `You are a voice assistant answering an inbound phone call.`;

  const assistantNameLine = `Your name is ${ASSISTANT_NAME}.`;
  const nameResponseLine = OPERATOR_NAME
    ? `If asked your name, say: 'I'm ${ASSISTANT_NAME}, ${OPERATOR_NAME}'s assistant.'`
    : `If asked your name, say: 'I'm ${ASSISTANT_NAME}.'`;

  const operatorRef = OPERATOR_NAME || 'the operator';

  const calendarRef = DEFAULT_CALENDAR ? `the ${DEFAULT_CALENDAR} calendar` : 'the calendar';

  return [
    assistantIntro,
    assistantNameLine,
    nameResponseLine,
    ...styleRules,
    `Start by introducing yourself as ${operatorRef}'s assistant.`,
    'Default mode is friendly conversation (NOT message-taking).',
    "Ask how they're doing (1 question). Optionally ask 1 brief follow-up if it feels natural.",
    "Then ask how you can help today.",
    '',
    'Message-taking (conditional):',
    "- Only take a message if the caller explicitly asks to leave a message / asks the operator to call them back / asks you to pass something along.",
    `- If the caller asks for ${operatorRef} directly (e.g., 'Is ${operatorRef} there?') and unavailable, offer ONCE: '${operatorRef === OPERATOR_NAME ? 'They are' : 'The operator is'} not available at the moment — would you like to leave a message?'`,
    '',
    'If taking a message:',
    "1) Ask for the caller's name.",
    "2) Ask for their callback number.",
    "   - If unclear, ask them to repeat it digit-by-digit.",
    `3) Ask for their message for ${operatorRef}.`,
    "4) Recap name + callback + message briefly.",
    `5) End politely: say you'll pass it along to ${operatorRef} and thank them for calling.`,
    '',
    'If NOT taking a message:',
    '- Continue a brief, helpful conversation aligned with what the caller wants.',
    '- If they are vague, ask one clarifying question, then either help or offer to take a message.',
    '',
    "Do not mention OpenAI, Twilio, SIP, models, prompts, or latency.",
    '',
    'Tools:',
    "- You have access to an ask_openclaw tool. Use it whenever the caller asks something you can't answer from your instructions alone.",
    '- Examples: checking availability, looking up info, booking appointments.',
    '- When calling ask_openclaw, say something natural like "Let me check on that" to fill the pause.',
    '',
    'Booking appointments:',
    `- When a caller wants to book an appointment with ${operatorRef}, first use ask_openclaw to check availability.`,
    "- Once the caller picks a time, DO NOT book it yet. First collect ALL of the following:",
    "  1) Caller's full name",
    "  2) Callback phone number",
    "  3) What the meeting is regarding (brief topic/purpose)",
    "- Only AFTER you have all three pieces of info, use ask_openclaw to create the event.",
    `- Include all collected info in the booking request. ${DEFAULT_CALENDAR ? `ALWAYS specify ${calendarRef}.` : ''} Example:`,
    DEFAULT_CALENDAR
      ? `  "Please create a calendar event on ${calendarRef}: Meeting with John Smith on Monday February 17 at 2:00 PM to 3:00 PM. Notes: interested in collaboration. Callback: 555-1234."`
      : `  "Please create a calendar event: Meeting with John Smith on Monday February 17 at 2:00 PM to 3:00 PM. Notes: interested in collaboration. Callback: 555-1234."`,
    "- Recap the details to the caller (name, time, topic) and confirm the booking AFTER the assistant confirms the event was created.",
    "- This is essential — never create a calendar event without the caller's name, number, and purpose.",
    '',
    'SUMMARY_JSON rule:',
    "- IMPORTANT: SUMMARY_JSON is metadata only. Do NOT speak it out loud. It must be completely silent.",
    "- Only emit SUMMARY_JSON if you actually took a message (not for appointment bookings).",
    "- Format: SUMMARY_JSON:{\"name\":\"...\",\"callback\":\"...\",\"message\":\"...\"}",
    "- This must be the absolute last output after the call ends. Never say it aloud to the caller."
  ].join("\n");
}

function buildInboundGreeting(args: { style: InboundCallScreeningStyle }): string {
  // Generic scripted greeting
  const operatorPart = OPERATOR_NAME ? `, ${OPERATOR_NAME}'s assistant` : '';
  const orgPart = ORG_NAME ? ` here at ${ORG_NAME}` : '';
  return `Hi! This is ${ASSISTANT_NAME}${operatorPart}${orgPart}. How can I help you today?`;
}

function buildOutboundCallInstructions(args: { objective: string; callPlan?: CallPlan }): string {
  const operatorRef = OPERATOR_NAME || 'the operator';

  const lines: string[] = [
    `You are ${operatorRef}'s assistant placing an outbound phone call.`,
    'Your job is to accomplish the stated objective. Do not switch into inbound screening / message-taking unless explicitly instructed.',
    'Be natural, concise, and human. Use a friendly tone.',
    'Do not mention OpenAI, Twilio, SIP, models, prompts, or latency.',
    '',
    'Objective (follow this):',
    args.objective,
  ];

  if (args.callPlan) {
    const cp = args.callPlan;
    lines.push('', '--- Reservation / Call Details ---');
    if (cp.purpose) lines.push(`Purpose: ${cp.purpose}`);
    if (cp.restaurantName) lines.push(`Restaurant: ${cp.restaurantName}`);
    if (cp.date) lines.push(`Date: ${cp.date}`);
    if (cp.time) lines.push(`Time: ${cp.time}`);
    if (cp.partySize) lines.push(`Party size: ${cp.partySize}`);
    if (cp.notes) lines.push(`Special requests: ${cp.notes}`);
    if (cp.customer) {
      lines.push('', 'Booking under:');
      if (cp.customer.name) lines.push(`  Name: ${cp.customer.name}`);
      if (cp.customer.phone) lines.push(`  Phone: ${cp.customer.phone}`);
      if (cp.customer.email) lines.push(`  Email: ${cp.customer.email}`);
    }
    lines.push('');
    lines.push('Use these details to complete the reservation. Only share customer contact info if the callee asks for it.');
    lines.push('If the requested date/time is unavailable, ask what alternatives they have and note them — do NOT confirm an alternative without checking.');
    lines.push('If a deposit or credit card is required:');
    lines.push(`  1) Ask: "Could you hold that appointment and I'll get ${operatorRef} to call you back with that info?"`);
    lines.push('  2) If yes, confirm what name/number to call back on and what the deposit amount is.');
    lines.push('  3) Thank them and end the call politely.');
    lines.push('  4) Do NOT provide any payment details yourself.');
  }

  lines.push(
    '',
    'Tools:',
    '- You have access to an ask_openclaw tool. Use it when you need information you don\'t have (e.g., checking availability, confirming preferences, looking up details).',
    '- When you call ask_openclaw, say something natural to the caller like "Let me check on that for you" — do NOT go silent.',
    '- Keep your question to the assistant short and specific.',
    '',
    'Rules:',
    `- If the callee asks who you are: say you are ${operatorRef}'s assistant calling on ${operatorRef}'s behalf.`,
    `- If the callee asks to leave a message for ${operatorRef}: only do so if it supports the objective; otherwise say you can pass along a note and keep it brief.`,
    '- If the callee seems busy or confused: apologize and offer to call back later, then end politely.',
  );

  return lines.join('\n');
}

function buildOutboundGreeting(args: { objective: string }): string {
  // Generic scripted outbound greeting
  const orgPart = ORG_NAME ? ` from ${ORG_NAME}` : '';
  return `Hi! This is ${ASSISTANT_NAME}${orgPart}. How are you doing today?`;
}

function buildSilenceFollowup(args: { mode: 'inbound' | 'outbound' }): string {
  // Single follow-up after ~3s of silence.
  return args.mode === 'inbound'
    ? 'Just let me know how I can help.'
    : 'No rush — I just wanted to check in. How are things?';
}

// ─── Phase C2: Function call handlers ───

async function handleAskOpenClaw(
  ws: WebSocket,
  callId: string,
  itemId: string,
  fnCallId: string,
  rawArgs: string,
  objective?: string,
  callPlan?: CallPlan,
  transcriptStream?: fs.WriteStream,
  writeJsonl?: (obj: unknown) => void,
): Promise<void> {
  const log = writeJsonl ?? (() => {});
  let question = '';
  let context = '';

  try {
    const args = JSON.parse(rawArgs);
    question = String(args?.question ?? '');
    context = String(args?.context ?? '');
  } catch {
    question = rawArgs;
  }

  log({
    type: 'c2.ask_openclaw.start',
    call_id: callId,
    received_at: new Date().toISOString(),
    question,
    context,
  });

  // Read current transcript for context
  let transcript = '';
  if (transcriptStream) {
    try {
      const transcriptPath = logsTranscriptPath(callId);
      if (fs.existsSync(transcriptPath)) {
        transcript = fs.readFileSync(transcriptPath, 'utf8');
      }
    } catch {}
  }

  try {
    const answer = await askOpenClaw(question, {
      callPlan,
      objective,
      transcript,
    });

    log({
      type: 'c2.ask_openclaw.done',
      call_id: callId,
      received_at: new Date().toISOString(),
      question,
      answer,
    });

    sendFunctionCallOutput(ws, fnCallId, JSON.stringify({ answer }));
  } catch (e: any) {
    log({
      type: 'c2.ask_openclaw.error',
      call_id: callId,
      received_at: new Date().toISOString(),
      error: e?.message ?? String(e),
    });

    const operatorRef = OPERATOR_NAME || 'the operator';
    sendFunctionCallOutput(
      ws,
      fnCallId,
      JSON.stringify({ answer: `I couldn't reach the assistant right now. Let me have ${operatorRef} get back to you on that.` })
    );
  }
}

function sendFunctionCallOutput(ws: WebSocket, callId: string, output: string): void {
  // Create the function call output conversation item
  const itemCreate = {
    type: 'conversation.item.create',
    item: {
      type: 'function_call_output',
      call_id: callId,
      output,
    },
  };
  ws.send(JSON.stringify(itemCreate));

  // Trigger the model to respond with the function output
  const responseCreate = {
    type: 'response.create',
  };
  ws.send(JSON.stringify(responseCreate));
}

// ─── Phase C2: OpenClaw consultation ───

/**
 * Ask the OpenClaw assistant a question. Tries OpenClaw gateway first (full assistant with tools),
 * falls back to a quick OpenAI Chat Completions call with call context.
 */
async function askOpenClaw(
  question: string,
  callContext?: { callPlan?: CallPlan; objective?: string; transcript?: string }
): Promise<string> {
  const timeoutMs = 20_000;

  // Try OpenClaw gateway API first (sends to assistant main session)
  if (OPENCLAW_GATEWAY_TOKEN) {
    try {
      const answer = await askOpenClawViaGateway(question, callContext, timeoutMs);
      if (answer) return answer;
    } catch (e) {
      console.error('askOpenClaw gateway failed, falling back to Chat Completions', e);
    }
  }

  // Fallback: quick Chat Completions call with call context
  return askOpenClawViaChatCompletions(question, callContext);
}

async function askOpenClawViaGateway(
  question: string,
  callContext: { callPlan?: CallPlan; objective?: string; transcript?: string } | undefined,
  timeoutMs: number
): Promise<string | null> {
  // Use OpenClaw's OpenAI-compatible /v1/chat/completions endpoint.
  // This runs a full assistant agent turn (with tools, memory, calendar access).
  const assistantClient = new OpenAI({
    apiKey: OPENCLAW_GATEWAY_TOKEN,
    baseURL: `${OPENCLAW_GATEWAY_URL}/v1`,
    timeout: timeoutMs,
  });

  const operatorRef = OPERATOR_NAME || 'the operator';

  const systemParts: string[] = [
    `Voice agent (${ASSISTANT_NAME}) is on a live phone call on ${operatorRef}'s behalf and needs your help.`,
    'Respond concisely (1-2 sentences max) — the caller is waiting on the line.',
    'Do NOT greet, do NOT add preamble. Just answer the question directly.',
  ];
  if (callContext?.objective) systemParts.push(`Call objective: ${callContext.objective}`);
  if (callContext?.callPlan) {
    const cp = callContext.callPlan;
    if (cp.restaurantName) systemParts.push(`Restaurant: ${cp.restaurantName}`);
    if (cp.date) systemParts.push(`Date: ${cp.date}`);
    if (cp.time) systemParts.push(`Time: ${cp.time}`);
    if (cp.partySize) systemParts.push(`Party size: ${cp.partySize}`);
    if (cp.notes) systemParts.push(`Notes: ${cp.notes}`);
  }
  if (callContext?.transcript) {
    const lastLines = callContext.transcript.split('\n').slice(-10).join('\n');
    systemParts.push(`\nRecent call transcript:\n${lastLines}`);
  }

  try {
    const completion = await assistantClient.chat.completions.create({
      model: 'openclaw:main',
      messages: [
        { role: 'system', content: systemParts.join('\n') },
        { role: 'user', content: question },
      ],
      // Use a stable user string so repeated calls within the same bridge session
      // can share context (OpenClaw derives session key from this).
      user: `sip-bridge-${ASSISTANT_NAME.toLowerCase()}`,
    });

    const content = completion.choices?.[0]?.message?.content?.trim();
    return content || null;
  } catch (e: any) {
    if (e?.name === 'AbortError' || e?.code === 'ETIMEDOUT') {
      console.warn('askOpenClawViaGateway timed out');
      return null;
    }
    throw e;
  }
}

async function askOpenClawViaChatCompletions(
  question: string,
  callContext?: { callPlan?: CallPlan; objective?: string; transcript?: string }
): Promise<string> {
  const operatorRef = OPERATOR_NAME || 'the operator';
  const operatorInfo = OPERATOR_NAME ? `  Name: ${OPERATOR_NAME}` : '';
  const phoneInfo = OPERATOR_PHONE ? `  Phone: ${OPERATOR_PHONE}` : '';
  const emailInfo = OPERATOR_EMAIL ? `  Email: ${OPERATOR_EMAIL}` : '';

  const systemParts: string[] = [
    `You are the AI assistant for ${operatorRef}.`,
    `A voice agent (${ASSISTANT_NAME}) is on a live phone call on ${operatorRef}'s behalf and needs quick info.`,
    'Respond in 1-2 concise sentences. The caller is waiting — be fast and direct.',
  ];

  if (operatorInfo || phoneInfo || emailInfo) {
    systemParts.push('');
    if (operatorInfo) systemParts.push(operatorInfo);
    if (phoneInfo) systemParts.push(phoneInfo);
    if (emailInfo) systemParts.push(emailInfo);
  }

  if (callContext?.objective) systemParts.push(`\nCall objective: ${callContext.objective}`);
  if (callContext?.callPlan) {
    const cp = callContext.callPlan;
    if (cp.restaurantName) systemParts.push(`Restaurant: ${cp.restaurantName}`);
    if (cp.date) systemParts.push(`Date: ${cp.date}`);
    if (cp.time) systemParts.push(`Time: ${cp.time}`);
    if (cp.partySize) systemParts.push(`Party size: ${cp.partySize}`);
    if (cp.notes) systemParts.push(`Notes: ${cp.notes}`);
  }

  if (callContext?.transcript) {
    const lastLines = callContext.transcript.split('\n').slice(-10).join('\n');
    systemParts.push(`\nRecent call transcript:\n${lastLines}`);
  }

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      max_tokens: 150,
      temperature: 0.3,
      messages: [
        { role: 'system', content: systemParts.join('\n') },
        { role: 'user', content: question },
      ],
    });
    const operatorRefFallback = OPERATOR_NAME || 'the operator';
    return completion.choices?.[0]?.message?.content?.trim() ?? `I'm not sure — let me have ${operatorRefFallback} get back to you on that.`;
  } catch (e) {
    console.error('askOpenClawViaChatCompletions error', e);
    const operatorRefFallback = OPERATOR_NAME || 'the operator';
    return `I'm not sure about that right now. Let me have ${operatorRefFallback} get back to you.`;
  }
}

function callPlanToObjective(cp?: CallPlan): string {
  if (!cp) return '';
  const parts: string[] = [];
  if (cp.purpose === 'restaurant_reservation' && cp.restaurantName) {
    parts.push(`Make a reservation at ${cp.restaurantName}`);
    if (cp.date) parts.push(`for ${cp.date}`);
    if (cp.time) parts.push(`at ${cp.time}`);
    if (cp.partySize) parts.push(`for ${cp.partySize} people`);
    if (cp.notes) parts.push(`(${cp.notes})`);
  } else {
    if (cp.purpose) parts.push(cp.purpose);
    if (cp.notes) parts.push(cp.notes);
  }
  return parts.join(' ') || '';
}

function getOutboundCallPlanForKey(key: string, nowMs: number): CallPlan | undefined {
  for (const [k, info] of outboundIntentByKey.entries()) {
    if (nowMs - info.createdAtMs > OUTBOUND_INTENT_RETENTION_MS) outboundIntentByKey.delete(k);
  }
  return outboundIntentByKey.get(key)?.callPlan;
}

function getOutboundObjectiveForKey(key: string, nowMs: number): string | undefined {
  // Cleanup first
  for (const [k, info] of outboundIntentByKey.entries()) {
    if (nowMs - info.createdAtMs > OUTBOUND_INTENT_RETENTION_MS) outboundIntentByKey.delete(k);
  }
  const rec = outboundIntentByKey.get(key);
  if (!rec) {
    console.log('OUTBOUND_OBJECTIVE_MISS', { key, mapSize: outboundIntentByKey.size });
  } else {
    console.log('OUTBOUND_OBJECTIVE_HIT', { key, hasObjective: !!rec.objective });
  }
  return rec?.objective?.trim() ? rec.objective.trim() : undefined;
}

function normalizePhoneLike(s: unknown): string {
  if (typeof s !== 'string') return '';
  return s.trim();
}

function rememberInboundCallFromTwilioBody(body: any): void {
  const callSid = normalizePhoneLike(body?.CallSid);
  const from = normalizePhoneLike(body?.From || body?.Caller);
  if (!callSid || !from) return;

  const nowMs = Date.now();
  inboundCallBySid.set(callSid, { callSid, from, receivedAtMs: nowMs });
  cleanupInboundCalls(nowMs);
}

function cleanupInboundCalls(nowMs: number): void {
  for (const [sid, info] of inboundCallBySid.entries()) {
    if (nowMs - info.receivedAtMs > INBOUND_CALL_RETENTION_MS) inboundCallBySid.delete(sid);
  }
}

function findRecentInboundCallForWebhook(event: any, nowMs: number): InboundCallInfo | undefined {
  cleanupInboundCalls(nowMs);

  const candidateSid = extractTwilioCallSidFromWebhook(event);
  if (candidateSid) {
    const info = inboundCallBySid.get(candidateSid);
    if (info && nowMs - info.receivedAtMs <= INBOUND_CALL_LOOKBACK_MS) return info;
  }

  let best: InboundCallInfo | undefined = undefined;
  for (const info of inboundCallBySid.values()) {
    if (nowMs - info.receivedAtMs > INBOUND_CALL_LOOKBACK_MS) continue;
    if (!best || info.receivedAtMs > best.receivedAtMs) best = info;
  }
  return best;
}

function extractTwilioCallSidFromWebhook(event: any): string | undefined {
  const directCandidates: unknown[] = [
    event?.data?.twilio?.CallSid,
    event?.data?.twilio?.call_sid,
    event?.data?.call?.twilio?.CallSid,
    event?.data?.call?.twilio?.call_sid,
    event?.data?.call?.metadata?.twilio_call_sid,
    event?.data?.metadata?.twilio_call_sid
  ];
  for (const c of directCandidates) {
    const s = normalizePhoneLike(c);
    if (s) return s;
  }

  // Newer webhook payload includes sip_headers: [{name,value}, ...]
  const sipHeadersArr = event?.data?.sip_headers;
  if (Array.isArray(sipHeadersArr)) {
    for (const h of sipHeadersArr) {
      const name = String(h?.name ?? '').toLowerCase();
      const value = normalizePhoneLike(h?.value);
      if (name === 'x-twilio-callsid' || name === 'x_twilio_callsid') {
        if (value) return value;
      }
    }
  }

  // Legacy/alternative shapes (kept just in case)
  const headers = event?.data?.sip?.headers ?? event?.data?.call?.sip?.headers;
  if (headers && typeof headers === 'object') {
    for (const [k, v] of Object.entries(headers)) {
      const kk = String(k).toLowerCase();
      if (kk === 'x-twilio-callsid' || kk === 'x_twilio_callsid') {
        const s = normalizePhoneLike(v);
        if (s) return s;
      }
    }
  }

  return undefined;
}

function extractBridgeIdFromWebhook(event: any): string | undefined {
  const sipHeadersArr = event?.data?.sip_headers;
  if (Array.isArray(sipHeadersArr)) {
    // Check for standalone header first
    for (const h of sipHeadersArr) {
      const name = String(h?.name ?? '').toLowerCase();
      const value = normalizePhoneLike(h?.value);
      if (name === 'x-bridge-id' || name === 'x_bridge_id') {
        if (value) return value;
      }
    }
    // Fall back: parse from To header's SIP URI parameters (;x_bridge_id=...)
    for (const h of sipHeadersArr) {
      const name = String(h?.name ?? '').toLowerCase();
      if (name === 'to') {
        const val = String(h?.value ?? '');
        const match = val.match(/x_bridge_id=([^;>\s]+)/i);
        if (match?.[1]) {
          const decoded = decodeURIComponent(match[1]);
          console.log('BRIDGE_ID_FROM_TO_HEADER', decoded);
          return decoded;
        }
      }
    }
  }
  return undefined;
}

function extractObjectiveFromWebhook(event: any): string | undefined {
  const sipHeadersArr = event?.data?.sip_headers;
  if (Array.isArray(sipHeadersArr)) {
    for (const h of sipHeadersArr) {
      const name = String(h?.name ?? '').toLowerCase();
      const value = normalizePhoneLike(h?.value);
      if (name === 'x-objective' || name === 'x_objective') {
        if (value) return value;
      }
    }
  }
  return undefined;
}


function selectInboundCallScreeningStyle(fromRaw: string | undefined): InboundCallScreeningStyle {
  const from = normalizePhoneLike(fromRaw);
  // Check against configured GenZ numbers
  if (GENZ_NUMBERS.includes(from)) return 'genz';
  return 'friendly';
}

function logsDir(): string {
  return path.join(process.cwd(), 'logs');
}

function safeCallId(callId: string): string {
  return callId.replace(/[^a-zA-Z0-9._-]/g, '_');
}

function logsJsonlPath(callId: string): string {
  return path.join(logsDir(), `${safeCallId(callId)}.jsonl`);
}

function logsTranscriptPath(callId: string): string {
  return path.join(logsDir(), `${safeCallId(callId)}.txt`);
}

function logsSummaryPath(callId: string): string {
  return path.join(logsDir(), `${safeCallId(callId)}.summary.json`);
}

function ensureDirSync(dir: string) {
  if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });
}

function endStream(stream: fs.WriteStream): Promise<void> {
  return new Promise((resolve) => {
    stream.end(() => resolve());
  });
}

function extractTranscriptStrings(event: any): string[] {
  // IMPORTANT: Do NOT recursively scrape all `text`/`transcript` fields.
  // That approach produces duplicates (same assistant transcript appears in multiple event shapes)
  // and often misses caller transcription events.
  //
  // Instead, explicitly extract from known realtime event types.

  const out: string[] = [];

  const pushLine = (speaker: 'CALLER' | string, text: unknown) => {
    if (typeof text !== 'string') return;
    const t = text.trim();
    if (!t) return;
    const line = `${speaker}: ${t}`;
    // simple de-dupe within a single event
    if (out[out.length - 1] === line) return;
    out.push(line);
  };

  const type = String(event?.type ?? '');

  // Caller speech transcription (input)
  // Common shapes:
  // - conversation.item.input_audio_transcription.completed
  //   { item: { role:'user', content:[{ transcript: '...' }] } }
  if (type === 'conversation.item.input_audio_transcription.completed') {
    const transcript =
      event?.item?.content?.[0]?.transcript ??
      event?.item?.content?.[0]?.text ??
      event?.transcript ??
      event?.text;
    pushLine('CALLER', transcript);
    return out;
  }

  // Assistant transcript events
  // Common shapes:
  // - response.audio_transcript.done { transcript: '...' }
  // - response.output_text.done { text: '...' }
  if (type === 'response.audio_transcript.done') {
    pushLine(ASSISTANT_NAME.toUpperCase(), event?.transcript);
    return out;
  }

  if (type === 'response.output_text.done') {
    pushLine(ASSISTANT_NAME.toUpperCase(), event?.text);
    return out;
  }

  // Fallback: older/alternate shapes where transcript is nested.
  // response.content_part.done may include { part: { transcript/text } }
  if (type === 'response.content_part.done') {
    const part = event?.part;
    if (part?.transcript) pushLine(ASSISTANT_NAME.toUpperCase(), part.transcript);
    else if (part?.text) pushLine(ASSISTANT_NAME.toUpperCase(), part.text);
    return out;
  }

  return out;
}

async function finalizeSummaryFromTranscript(callId: string): Promise<void> {
  try {
    const transcriptPath = logsTranscriptPath(callId);
    const summaryPath = logsSummaryPath(callId);
    if (!fs.existsSync(transcriptPath)) return;

    const text = await fs.promises.readFile(transcriptPath, 'utf8');
    const summary = parseSummaryJsonFromTranscript(text);
    if (!summary) return;

    await fs.promises.writeFile(summaryPath, `${JSON.stringify(summary, null, 2)}\n`, 'utf8');
  } catch (e) {
    console.error('finalizeSummaryFromTranscript error', callId, e);
  }
}

function parseSummaryJsonFromTranscript(transcript: string): any | null {
  const marker = 'SUMMARY_JSON:';
  const lines = transcript.split(/\r?\n/);
  for (let i = lines.length - 1; i >= 0; i--) {
    const line = lines[i].trim();
    if (!line) continue;
    const idx = line.indexOf(marker);
    if (idx === -1) continue;
    const after = line.slice(idx + marker.length).trim();
    try {
      const obj = JSON.parse(after);
      if (obj && typeof obj === 'object') return obj;
    } catch {
      return null;
    }
  }
  return null;
}
